{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('New_articles_combined.csv')\n",
    "num_rows = len(df)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Article'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "import string\n",
    "punct = string.punctuation\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA(train_df):\n",
    "    # Missing values\n",
    "    missing_values_check = train_df.isnull().sum()\n",
    "    print(\"Missing values\", missing_values_check)\n",
    "    # checking tweets which are blank completely\n",
    "    blanks = []\n",
    "    for index in train_df['Clean_text']:\n",
    "        if index.isspace() == True:\n",
    "            blanks.append(index)\n",
    "    print(\"Blanks\")\n",
    "    print(blanks)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove urls\n",
    "    url_pattern = re.compile(r'http\\S+')\n",
    "    tokens = [token for token in tokens if not url_pattern.match(token)]\n",
    "    # remove punct\n",
    "    # remove stop words\n",
    "    # lemmatize\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens if t not in stop_words and t not in punct]\n",
    "    return tokens\n",
    "\n",
    "def clean(text):\n",
    "    cleantext = re.sub(r'<.*?>', ' ', str(text))\n",
    "    cleantext = re.sub(r'\\d+',' ', cleantext)\n",
    "    cleantext = re.sub(r'[^a-z\\s]', ' ', cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def preprocessing(df):\n",
    "    df.loc[:, 'Clean_text'] = df.loc[:, 'Clean_text'].str.lower()\n",
    "    print(\"lower over\")\n",
    "    df.loc[:, 'Clean_text'] = df.loc[:, 'Clean_text'].apply(lambda x: re.sub(r'[\\r\\n]', '', x))\n",
    "    print(\"\\r\\n over\")\n",
    "    df.loc[:, 'Clean_text'] = df.loc[:, 'Clean_text'].apply(clean)\n",
    "    print(\"regex over\")\n",
    "    df.loc[:, 'Clean_text'] = df.loc[:, 'Clean_text'].apply(tokenize)\n",
    "    print(\"toenize over\")\n",
    "    df.loc[:, 'Clean_text'] = df.loc[:, 'Clean_text'].apply(lambda x: ' '.join(x))\n",
    "    print(\"join token\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower over\n",
      "\n",
      " over\n",
      "regex over\n",
      "toenize over\n",
      "join token\n",
      "   Unnamed: 0                                            Article  \\\n",
      "0           0  The CBI on Saturday booked four former officia...   \n",
      "1           1  Chief Justice JS Khehar has said the Supreme C...   \n",
      "2           2  At least three people were killed, including a...   \n",
      "3           3  Mukesh Ambani-led Reliance Industries (RIL) wa...   \n",
      "4           4  TV news anchor Arnab Goswami has said he was t...   \n",
      "\n",
      "                                          Clean_text  \n",
      "0  cbi saturday booked four former official syndi...  \n",
      "1  chief justice j khehar said supreme court go p...  \n",
      "2  least three people killed including policeman ...  \n",
      "3  mukesh ambani led reliance industry ril barred...  \n",
      "4  tv news anchor arnab goswami said told could p...  \n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "file_path = \"New_articles_combined.csv\"\n",
    "# Read dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    #return\n",
    "\n",
    "# Check if 'Articles' column exists\n",
    "if 'Article' not in df.columns:\n",
    "    print(\"Error: Dataset must contain 'Articles' column\")\n",
    "    #return\n",
    "    \n",
    "# Remove any null values\n",
    "df = df.dropna(subset=['Article'])\n",
    "df['Clean_text'] = df['Article']\n",
    "#EDA(df)\n",
    "df = preprocessing(df)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "df.to_csv('data_preprocessed.csv')\n",
    "\n",
    "# if '__main__' == __name__:\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
