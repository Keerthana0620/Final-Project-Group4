{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X938Go882Sqn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0U0z269QCIqW"},"source":["## Creating CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_6r5qCy77HN"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dxPw8sR8B6i"},"outputs":[],"source":["import pandas as pd\n","dataset_path = '/content/gdrive/My Drive/Keerthana/reduced_dataset.csv'\n","df = pd.read_csv(dataset_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJbbXSn8ePzc"},"outputs":[],"source":["import pandas as pd\n","dataset_path = 'reduced_dataset.csv'\n","df = pd.read_csv(dataset_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLrJiJt-B1Np"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKOOjBBQB9AA"},"outputs":[],"source":["df.rename(columns={'cleaned_text': 'Article'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzwXKnsI8Da5"},"outputs":[],"source":["df.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)\n","df.drop('Unnamed: 0.1', axis=1, inplace=True)\n","df.drop('Clean_text', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGqNS8N7-469"},"outputs":[],"source":["df.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk7OIlcS8lRp"},"outputs":[],"source":["df.to_csv('/content/gdrive/My Drive/Keerthana/data_reduced.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BYq-dKvePzg"},"outputs":[],"source":["df.to_csv('data_reduced.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"5Gn1kMMP2fCY"},"source":["## Proprocessing of Articles for generation. To maintain meaning as cleaned_text and complete cleaning as search_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16O8TC5MUJs3"},"outputs":[],"source":["pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLCwxxvkePzh"},"outputs":[],"source":["pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fWl41BY2eGa"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import contractions\n","import html\n","from tqdm import tqdm\n","import logging\n","from typing import Optional"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-cFHE-TUyig","outputId":"893ebfe4-d048-42d7-9e07-9dc29e2bde03"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L67lUv912sty"},"outputs":[],"source":["class StreamlinedNewsPreprocessor:\n","    def __init__(self):\n","        \"\"\"\n","        Initialize the preprocessor with minimal required NLTK components\n","        \"\"\"\n","        # Download required NLTK data\n","        try:\n","            nltk.download('punkt')\n","            nltk.download('stopwords')\n","            nltk.download('wordnet')\n","        except:\n","            pass\n","\n","        self.lemmatizer = WordNetLemmatizer()\n","        self.stop_words = set(stopwords.words('english'))\n","\n","        logging.basicConfig(level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","    def basic_clean(self, text: str) -> str:\n","        \"\"\"\n","        Basic cleaning for generation - preserves meaning and stop words\n","        \"\"\"\n","        if not isinstance(text, str):\n","            return \"\"\n","\n","        # Convert to lowercase\n","        text = text.lower().strip()\n","\n","        # Decode HTML entities\n","        text = html.unescape(text)\n","\n","        # Expand contractions\n","        text = contractions.fix(text)\n","\n","        # Remove URLs\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","\n","        # Remove email addresses\n","        text = re.sub(r'\\S+@\\S+', '', text)\n","\n","        # Remove special characters but keep sentence structure\n","        text = re.sub(r'[^\\w\\s.,!?]', ' ', text)\n","\n","        # Remove multiple spaces and newlines\n","        text = ' '.join(text.split())\n","\n","        return text\n","\n","    def search_clean(self, text: str) -> str:\n","        \"\"\"\n","        Aggressive cleaning for semantic search - removes stop words and lemmatizes\n","        \"\"\"\n","        if not isinstance(text, str):\n","            return \"\"\n","\n","        # Basic cleaning first\n","        text = self.basic_clean(text)\n","\n","        # Tokenize\n","        tokens = word_tokenize(text)\n","\n","        # Remove stop words and lemmatize\n","        tokens = [self.lemmatizer.lemmatize(token.lower()) for token in tokens\n","                 if token.lower() not in self.stop_words]\n","\n","        return ' '.join(tokens)\n","\n","    def preprocess_dataframe(self,\n","                           df: pd.DataFrame,\n","                           text_column: str,\n","                           batch_size: int = 1000,\n","                           output_path: Optional[str] = None) -> pd.DataFrame:\n","        \"\"\"\n","        Preprocess the dataframe keeping only essential columns for generation and search\n","\n","        Parameters:\n","        -----------\n","        df : pd.DataFrame\n","            Input dataframe containing articles\n","        text_column : str\n","            Name of the column containing article text\n","        batch_size : int\n","            Size of batches for processing\n","        output_path : Optional[str]\n","            If provided, saves the processed dataframe to this path\n","\n","        Returns:\n","        --------\n","        pd.DataFrame\n","            Processed dataframe with only cleaned_text and search_text columns\n","        \"\"\"\n","        print(\"Starting preprocessing...\")\n","\n","        # Create new dataframe with only required columns\n","        processed_df = pd.DataFrame()\n","\n","        # Process in batches\n","        for i in tqdm(range(0, len(df), batch_size)):\n","            batch = df[text_column].iloc[i:i+batch_size].copy()\n","\n","            # Create temporary dataframe for batch processing\n","            temp_df = pd.DataFrame({\n","                'cleaned_text': batch.apply(self.basic_clean),\n","                'search_text': batch.apply(self.search_clean)\n","            })\n","\n","            # Append to processed dataframe\n","            processed_df = pd.concat([processed_df, temp_df], ignore_index=True)\n","\n","        # Remove rows where either column is empty\n","        processed_df = processed_df.dropna(subset=['cleaned_text', 'search_text'])\n","        processed_df = processed_df[processed_df['cleaned_text'].str.strip() != '']\n","        processed_df = processed_df[processed_df['search_text'].str.strip() != '']\n","\n","        self.logger.info(f\"Preprocessing completed! Shape: {processed_df.shape}\")\n","\n","        # Save if output path is provided\n","        if output_path:\n","            processed_df.to_csv(output_path, index=False)\n","            self.logger.info(f\"Processed data saved to {output_path}\")\n","\n","        return processed_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwQWFuSp2zGu"},"outputs":[],"source":["def verify_processed_data(df: pd.DataFrame, sample_size: int = 3) -> None:\n","    \"\"\"\n","    Verify the processed data by printing sample comparisons\n","    \"\"\"\n","    print(\"\\nSample Comparisons (Original vs Cleaned vs Search):\")\n","    samples = df.sample(n=min(sample_size, len(df)))\n","\n","    for idx, row in samples.iterrows():\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"Cleaned Text: {row['cleaned_text'][:200]}...\")\n","        print(f\"Search Text: {row['search_text'][:200]}...\")\n","        print(\"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bLUbNXw23cO","outputId":"f335de48-b760-491c-e013-dc85c4d60289"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Sunder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Starting preprocessing...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 165/165 [08:54<00:00,  3.24s/it]\n","INFO:__main__:Preprocessing completed! Shape: (164511, 2)\n","INFO:__main__:Processed data saved to reduced_processed_news_essential.csv\n"]},{"name":"stdout","output_type":"stream","text":["\n","Sample Comparisons (Original vs Cleaned vs Search):\n","\n","================================================================================\n","Cleaned Text: by . sara smyth . published . est december . . updated . est december . modernising once reliant on victorian buildings prison governors now have phones in every cell . prisoners in newly built jails ...\n","Search Text: . sara smyth . published . est december . . updated . est december . modernising reliant victorian building prison governor phone every cell . prisoner newly built jail phone computer terminal cell ab...\n","================================================================================\n","\n","================================================================================\n","Cleaned Text: foreign portfolio investors have bought . billion around crore worth of indian equities in march so far the highest in months. india received the third largest inflow in asia after taiwan and south ko...\n","Search Text: foreign portfolio investor bought . billion around crore worth indian equity march far highest month . india received third largest inflow asia taiwan south korea march . earlier year fpis pulled . bi...\n","================================================================================\n","\n","================================================================================\n","Cleaned Text: indias largest two wheeler manufacturer hero motocorp on thursday reported a standalone net profit of . crore for the quarter ended march up by . over the corresponding period a year ago. the rise was...\n","Search Text: india largest two wheeler manufacturer hero motocorp thursday reported standalone net profit . crore quarter ended march . corresponding period year ago . rise attributed . increase quarterly sale hig...\n","================================================================================\n","\n","Processed DataFrame Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 164511 entries, 0 to 164510\n","Data columns (total 2 columns):\n"," #   Column        Non-Null Count   Dtype \n","---  ------        --------------   ----- \n"," 0   cleaned_text  164511 non-null  object\n"," 1   search_text   164511 non-null  object\n","dtypes: object(2)\n","memory usage: 2.5+ MB\n","None\n","\n","Memory Usage: 740.00 MB\n"]}],"source":["if __name__ == \"__main__\":\n","    # Initialize preprocessor\n","    preprocessor = StreamlinedNewsPreprocessor()\n","\n","    # Read data\n","    dataset_path = 'data_reduced.csv'\n","    df = pd.read_csv(dataset_path)\n","\n","    # Preprocess data\n","    processed_df = preprocessor.preprocess_dataframe(\n","        df=df,\n","        text_column='Article',\n","        output_path='reduced_processed_news_essential.csv'\n","    )\n","\n","    # Verify the processed data\n","    verify_processed_data(processed_df)\n","\n","    print(\"\\nProcessed DataFrame Info:\")\n","    print(processed_df.info())\n","\n","    # Memory usage\n","    memory_usage = processed_df.memory_usage(deep=True).sum() / 1024**2\n","    print(f\"\\nMemory Usage: {memory_usage:.2f} MB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCO07jyRUQEb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Fkpgo46MePzm"},"source":["# Preprocessing according to BART generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAqSGF9OePzn"},"outputs":[],"source":["import pandas as pd\n","import re\n","from pathlib import Path\n","import json\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYpIYBZpePzo"},"outputs":[],"source":["\n","\n","class NewsDataPreprocessor:\n","    def __init__(self, input_file, output_dir):\n","        self.input_file = input_file\n","        self.output_dir = Path(output_dir)\n","        self.output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def clean_text(self, text):\n","        \"\"\"Basic text cleaning\"\"\"\n","        if not isinstance(text, str):\n","            return \"\"\n","\n","        # Remove formatting artifacts and metadata\n","        patterns = [\n","            (r'\\s*\\.\\s*\\|\\s*\\.\\s*', ' '), # Remove '. | .'\n","            (r'By\\s*\\.\\s*[^.]*\\.\\s*PUBLISHED:\\s*[^.]*\\.\\s*UPDATED:[^.]*\\d{4}\\s*', ''), # Remove metadata\n","            (r'\\s+', ' '), # Remove extra whitespace\n","            (r'[\\n\\t]', ' '), # Remove newlines and tabs\n","            (r'\\s+\\.', '.'), # Fix spaces before periods\n","            (r'\\s+,', ','), # Fix spaces before commas\n","            (r'\\s*\"\\s*', '\"'), # Standardize quotes spacing\n","            (r'\\s*\\'\\s*', \"'\"), # Standardize apostrophes (fixed)\n","            (r'\\.+', '.'), # Remove multiple periods\n","        ]\n","\n","        for pattern, replacement in patterns:\n","            text = re.sub(pattern, replacement, text)\n","\n","        return text.strip()\n","\n","    def clean_highlights(self, highlights):\n","        \"\"\"Clean highlights specifically\"\"\"\n","        if not isinstance(highlights, str):\n","            return \"\"\n","\n","        # Remove bullet points and merge lines\n","        highlights = re.sub(r'^\\s*•\\s*', '', highlights)\n","        highlights = ' '.join(highlights.split('\\n'))\n","        highlights = self.clean_text(highlights)\n","\n","        # Remove trailing dots and ensure proper punctuation\n","        highlights = re.sub(r'\\s*\\.\\s*$', '', highlights)\n","        highlights = re.sub(r'\\s*\\.\\s*(?=\\S)', '. ', highlights)\n","\n","        return highlights.strip()\n","\n","    def extract_key_phrases(self, highlight):\n","        \"\"\"Extract key phrases from highlight\"\"\"\n","        # Extract main topics/entities\n","        key_words = set()\n","        # Add main nouns/topics\n","        key_words.update(re.findall(r'\\b[A-Z][a-z]+\\b', highlight))\n","        # Add important words\n","        key_words.update(re.findall(r'\\b\\w+(?:essay|news|photo|report)\\b', highlight.lower()))\n","        return ' '.join(list(key_words)[:3])\n","\n","    def create_bart_format(self, row):\n","        \"\"\"Create multiple training examples with varying input lengths\"\"\"\n","        cleaned_article = self.clean_text(row['article'])\n","        full_highlight = self.clean_highlights(row['highlights'])\n","\n","        if len(cleaned_article) < 50:  # Minimum article length\n","            return None\n","\n","        training_examples = []\n","\n","        # 1. Full highlight version\n","        if len(full_highlight) >= 10:\n","            training_examples.append({\n","                'input_text': full_highlight,\n","                'target_text': cleaned_article\n","            })\n","\n","        # 2. Short keyword version\n","        keywords = self.extract_key_phrases(full_highlight)\n","        if len(keywords) >= 3:\n","            training_examples.append({\n","                'input_text': keywords,\n","                'target_text': cleaned_article\n","            })\n","\n","        # 3. Topic-only version\n","        main_topic = ' '.join(re.findall(r'\\b[A-Z][a-z]+\\b', full_highlight)[:2])\n","        if main_topic:\n","            training_examples.append({\n","                'input_text': main_topic + ' news',\n","                'target_text': cleaned_article\n","            })\n","\n","        return training_examples\n","\n","    def process_and_split_data(self, test_size=0.1, val_size=0.1, random_state=42):\n","        \"\"\"Process the data and split into train/val/test sets\"\"\"\n","        print(\"Reading data...\")\n","        df = pd.read_csv(self.input_file)\n","\n","        print(\"Preprocessing articles and highlights...\")\n","        processed_data = []\n","        for _, row in tqdm(df.iterrows(), total=len(df)):\n","            processed_item = self.create_bart_format(row)\n","            if processed_item:\n","                processed_data.extend(processed_item)\n","\n","        # First split: separate test set\n","        train_val_data, test_data = train_test_split(\n","            processed_data,\n","            test_size=test_size,\n","            random_state=random_state\n","        )\n","\n","        # Second split: separate validation set from training set\n","        val_adjusted_size = val_size / (1 - test_size)\n","        train_data, val_data = train_test_split(\n","            train_val_data,\n","            test_size=val_adjusted_size,\n","            random_state=random_state\n","        )\n","\n","        # Save splits\n","        splits = {\n","            'train': train_data,\n","            'validation': val_data,\n","            'test': test_data\n","        }\n","\n","        for split_name, split_data in splits.items():\n","            output_file = self.output_dir / f'{split_name}.json'\n","            with open(output_file, 'w', encoding='utf-8') as f:\n","                json.dump(split_data, f, indent=2, ensure_ascii=False)\n","\n","            print(f\"{split_name} set size: {len(split_data)}\")\n","\n","        return len(processed_data)\n","\n","    def validate_splits(self):\n","        \"\"\"Validate the created splits\"\"\"\n","        issues = []\n","        for split in ['train', 'validation', 'test']:\n","            file_path = self.output_dir / f'{split}.json'\n","            if not file_path.exists():\n","                issues.append(f\"{split} file not found\")\n","                continue\n","\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Validate sample entries\n","            for idx, item in enumerate(data[:5]):\n","                if 'input_text' not in item or 'target_text' not in item:\n","                    issues.append(f\"Missing required fields in {split} set, item {idx}\")\n","                if not item['input_text'] or not item['target_text']:\n","                    issues.append(f\"Empty text in {split} set, item {idx}\")\n","\n","        return issues\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqAXmqn1ePzq","outputId":"5b8bd329-ba00-43a2-af36-b86e9e94cffe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading data...\n","Preprocessing articles and highlights...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 287113/287113 [02:53<00:00, 1656.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["train set size: 688864\n","validation set size: 86109\n","test set size: 86109\n","\n","Total processed examples: 861082\n","\n","All splits validated successfully!\n"]}],"source":["# Usage example\n","def main():\n","    # Configure paths\n","    input_file = 'train.csv'  # Update with your input file path\n","    output_dir = 'processed_data'        # Update with your desired output directory\n","\n","    # Initialize preprocessor\n","    preprocessor = NewsDataPreprocessor(input_file, output_dir)\n","\n","    # Process and split data\n","    total_processed = preprocessor.process_and_split_data()\n","    print(f\"\\nTotal processed examples: {total_processed}\")\n","\n","    # Validate the splits\n","    issues = preprocessor.validate_splits()\n","    if issues:\n","        print(\"\\nValidation issues found:\")\n","        for issue in issues:\n","            print(f\"- {issue}\")\n","    else:\n","        print(\"\\nAll splits validated successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"Wbq0rCpKePzr"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}